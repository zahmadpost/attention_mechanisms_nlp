{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matchzoo as mz\n",
    "from matchzoo import DataPack\n",
    "from matchzoo.engine.base_preprocessor import BasePreprocessor\n",
    "from matchzoo.preprocessors.units.unit import Unit\n",
    "from matchzoo.engine.base_model import BaseModel\n",
    "from matchzoo.preprocessors import units\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim.downloader as gensim\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTextMatchingModel(BaseModel):\n",
    "       \n",
    "    def build(self):\n",
    "       \n",
    "        def select_weight(docs):\n",
    "            #both inputs have same size: [batchsize, 30, 300]\n",
    "            query, value = docs\n",
    "           \n",
    "            #reshape inputs\n",
    "            #query reshaped size is [batchsize, 30, 1, 300]\n",
    "            query_reshaped = tf.expand_dims(query, axis=2)\n",
    "            #value reshaped size is [batchsize, 1, 30, 300]\n",
    "            value_reshaped = tf.expand_dims(value, axis=1)\n",
    "           \n",
    "            #sum of query reshaped and value reshaped (broadcasts dimensions 1 and 2 (0-indexed))\n",
    "            #size of tensor c is [batchsize, 30, 30, 300]\n",
    "            sum_ = tf.add(query_reshaped,value_reshaped)\n",
    "           \n",
    "            #rectified version of sum_\n",
    "            #same shape as sum_\n",
    "            rectified = tf.math.tanh(sum_)\n",
    "\n",
    "            #calculate scores for each word pairing\n",
    "            #shape of scores is [batchsize, 30, 30, 1]\n",
    "            scores = keras.layers.Dense(1,activation='linear')(rectified)\n",
    "           \n",
    "            #each query word (each word in the document text) is given a weighting over the value words (each word in the query text)\n",
    "            #weights has same shape as scores [batchsize, 30, 30, 1]\n",
    "            weights = tf.nn.softmax(scores,axis=2)\n",
    "\n",
    "            #multiply the weights and the values\n",
    "            #value reshaped size is [batchsize, 1, 30, 300]\n",
    "            #weights has same shape as scores [batchsize, 30, 30, 1]\n",
    "            #weighted values has shape [batchsize, 30, 30, 300]\n",
    "            weighted_values = keras.layers.Multiply()([weights, value_reshaped])\n",
    "           \n",
    "            #output has shape of [batchsize, 30, 300]\n",
    "            output = tf.reduce_sum(weighted_values,axis=2)\n",
    "\n",
    "            return output\n",
    "\n",
    "        def select_weight_shape(shapes):\n",
    "            shape1, shape2 = shapes\n",
    "            return (shape1[0],30, 300) #(batch size, length of words (hard coded for simplification), length of embeddings)\n",
    "       \n",
    "        '''Inputs are left and right text'''\n",
    "        input_left = keras.Input(name='text_left', shape=(300,30))\n",
    "        input_right= keras.Input(name='text_right', shape=(300,30))\n",
    "       \n",
    "        '''Attention layer'''\n",
    "        Weighted_right = keras.layers.Lambda(select_weight,\n",
    "                    output_shape=select_weight_shape)([input_left, input_right])\n",
    "       \n",
    "        '''Flatten matrices before feeding to dense layer'''\n",
    "        flatten_layer = keras.layers.Flatten()\n",
    "        flat_left = flatten_layer(input_left)\n",
    "        flat_right = flatten_layer(Weighted_right)\n",
    "       \n",
    "        '''Pass through a dense layer for dimensionality reduction'''\n",
    "        dense = keras.layers.Dense(\n",
    "        64, activation='elu', use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros', kernel_regularizer=None,\n",
    "        bias_regularizer=None, activity_regularizer=None\n",
    "        )\n",
    "       \n",
    "        dense_left_result = dense(flat_left)\n",
    "\n",
    "        dense_right_result = dense(flat_right)\n",
    "       \n",
    "        '''Calculate cosine similarity'''\n",
    "        dotted = keras.layers.Dot(axes=1, normalize=True)([dense_left_result, dense_right_result])\n",
    "       \n",
    "        self._backend = keras.Model(inputs=[input_left,input_right], outputs=dotted, name=\"attention_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembeddingmodel = gensim.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedLengthUnit(Unit):\n",
    "    def __init__(self, fixed_length: int):\n",
    "        self._fixed_length = fixed_length\n",
    "   \n",
    "    def transform(self, input_: tf.Tensor) -> tf.Tensor:\n",
    "        input_length = input_.shape[1]\n",
    "        if input_length == 0:\n",
    "            input_ = tf.zeros([300,0])\n",
    "        pre_ct = (self._fixed_length-input_length)//2\n",
    "        post_ct = self._fixed_length-pre_ct-input_length\n",
    "        pre = tf.zeros([300,pre_ct])\n",
    "        post = tf.zeros([300,post_ct])\n",
    "        return tf.concat([pre, input_, post], axis=1)\n",
    "\n",
    "class AttentionModelPreprocessor(BasePreprocessor):\n",
    "    def __init__(self, fixed_length: int):\n",
    "        super().__init__()\n",
    "        self._fixed_length= fixed_length\n",
    "        self._fixedlength_unit = FixedLengthUnit(self._fixed_length)\n",
    "        self._tokenizer = RegexpTokenizer('[^\\d\\W]+')\n",
    "    \n",
    "   \n",
    "    def _filter_vocab(self, input_: list) ->list:\n",
    "        return list(word for word in input_ if word in wordembeddingmodel.vocab)\n",
    "       \n",
    "    def _process(self, input_: list) ->list:\n",
    "        #get word embeddings\n",
    "        vectors = list(wordembeddingmodel[word] for word in input_)\n",
    "        vectors = tf.transpose(tf.convert_to_tensor(vectors))\n",
    "        return vectors\n",
    "    \n",
    "    def fit(self, data_pack: DataPack, verbose: int=1):\n",
    "        return self\n",
    "   \n",
    "    def transform(self, data_pack: DataPack, verbose: int=1) -> DataPack:\n",
    "        data_pack = data_pack.copy()\n",
    "        data_pack.apply_on_text(self._tokenizer.tokenize, inplace=True, verbose=verbose)\n",
    "        data_pack.apply_on_text(self._filter_vocab, inplace=True, verbose=verbose)\n",
    "        data_pack.apply_on_text(self._process, inplace=True, verbose=verbose)\n",
    "        data_pack.apply_on_text(self._fixedlength_unit.transform, inplace=True, verbose=verbose)\n",
    "        return data_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preprocessor = AttentionModelPreprocessor(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"matchzoo\\international-goldstandard.csv\")\n",
    "train_size = .8\n",
    "valid_size = .1\n",
    "test_size = .1\n",
    "train_offset = int(len(df)*train_size)\n",
    "valid_offset = int(len(df)*(valid_size) + train_offset)\n",
    "test_offset = int(len(df)*(test_size))\n",
    "train_df = pd.DataFrame(df.iloc[0:train_offset])\n",
    "valid_df = pd.DataFrame(df.iloc[train_offset: valid_offset])\n",
    "test_df = pd.DataFrame(df.iloc[valid_offset:])\n",
    "valid_df.reset_index(inplace = True, drop=True)\n",
    "test_df.reset_index(inplace =True, drop=True)\n",
    "train_pack = mz.pack(train_df)\n",
    "valid_pack = mz.pack(valid_df)\n",
    "test_pack = mz.pack(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pack_processed = preprocessor.transform(train_pack, verbose=0)\n",
    "valid_pack_processed = preprocessor.transform(valid_pack, verbose=0)\n",
    "test_pack_processed = preprocessor.transform(test_pack, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_task = mz.tasks.Ranking(loss=mz.losses.RankHingeLoss())\n",
    "ranking_task.metrics = [\n",
    "        mz.metrics.MeanAveragePrecision(),\n",
    "        mz.metrics.MeanReciprocalRank(),\n",
    "        mz.metrics.NormalizedDiscountedCumulativeGain(k=1),\n",
    "        mz.metrics.NormalizedDiscountedCumulativeGain(k=3),\n",
    "        mz.metrics.NormalizedDiscountedCumulativeGain(k=5)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = mz.DataGenerator(\n",
    "            train_pack_processed,\n",
    "            mode='pair',\n",
    "            num_dup=5,\n",
    "            num_neg=1,\n",
    "            batch_size=10\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionTextMatchingModel()\n",
    "model.params['task'] = ranking_task\n",
    "model.build()\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x, valid_y = valid_pack_processed.unpack()\n",
    "evaluate = mz.callbacks.EvaluateAllMetrics(model, x=valid_x, y=valid_y, batch_size=len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator, epochs=10, callbacks=[evaluate], workers=4, use_multiprocessing=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = test_pack_processed.unpack()\n",
    "\n",
    "performances=model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predictions'] = model.predict(test_x)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(‘AttentionModelResults.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
